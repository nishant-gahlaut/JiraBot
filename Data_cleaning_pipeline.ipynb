{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('jira_tickets_cache.csv')\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re # We'll need this for regex operations later\n",
    "summary_col = 'summary'\n",
    "description_col = 'description'\n",
    "comments_col = 'comments' # We'll use this later\n",
    "\n",
    "# Create new columns for cleaned text to keep originals safe\n",
    "# Fill NaN (missing) values with an empty string before any cleaning\n",
    "df['cleaned_summary'] = df[summary_col].fillna('').astype(str)\n",
    "df['cleaned_description'] = df[description_col].fillna('').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-processing 'cleaned_comments' with KEYWORD FILTERING and all subsequent cleaning...\n",
      "Step 1 (parse_filter_and_format_comments) for comments: Done\n",
      "\n",
      "Original and cleaned 'comments' (with timestamp sorting):\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    comments  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         []   \n",
      "1  [{\"author\": \"Manish Mishra\", \"timestamp\": \"2025-04-01T13:16:26.359+0530\", \"cleaned_body\": \"Please Note : Same Behavior is observed for all brands. Titan was an example for clarification.\", \"mentions\": []}, {\"author\": \"Suresh Moturi\", \"timestamp\": \"2025-04-01T15:20:34.150+0530\", \"cleaned_body\": \"Hi  ,\\n\\nPlease look into\\n\\nFYI :\", \"mentions\": [\"712020:e7d668da-7257-4e25-8ebf-bf7a54deec2a\", \"712020:b7de7d8b-a82d-4628-817b-3433e570eb61\"]}, {\"author\": \"Harshitha Addagarla\", \"timestamp\": \"2025-04-02T17:06:08.913+0530\", \"cleaned_body\": \"hi  ,\\n\\nThe current system setup, as shown in the screenshot, does not allow storing or setting custom notes for tier downgrades. Right now, it only logs predefined notes like {{\\\"pointsenginetierdowngrade visits\\\"}}, which is set in bulk during tier related bulk jobs. We might need an enhancement to allow multiple custom notes to be set for different downgrade scenarios.\\n\\n!Screenshot 2025-04-02 at 4.43.13\\u202fPM.png|width=100%,alt=\\\"Screenshot 2025-04-02 at 4.43.13\\u202fPM.png\\\"!\\n\\n  can you please look into this further\\n\\nCC   \\n\\nRegards,\\n\\nHarshitha A\", \"mentions\": [\"557058:916077b0-83f3-4697-ba71-ecabb879dd42\", \"63e36ef1c3eb74ad8e99682b\", \"557058:a46004b7-b075-468c-be4c-75e0177a4d78\", \"712020:b7de7d8b-a82d-4628-817b-3433e570eb61\"]}, {\"author\": \"Manish Mishra\", \"timestamp\": \"2025-04-02T17:19:38.850+0530\", \"cleaned_body\": \"Hi  ,\\n\\nI am not aligned on this. When bulk job is evaluating customers eligibility based on strategy config which supports tracker (points or spend) then Notes should be auto build on top of that. It looks like Enhancement in strategy happened along with Bulk Job reading those configs but we still follows 10 year old logic for Notes. \\n\\nIt seems that this part was ignored in all enhancements happened in past.\", \"mentions\": [\"712020:e7d668da-7257-4e25-8ebf-bf7a54deec2a\"]}, {\"author\": \"Bhuvaneshwari Seshachalam\", \"timestamp\": \"2025-04-03T16:04:26.593+0530\", \"cleaned_body\": \"Hi  cc  Please provide your suggestions on this request.\", \"mentions\": [\"63e36ef1c3eb74ad8e99682b\", \"5af996675f0d5b06c28aa5d1\"]}, {\"author\": \"Naresh Silla\", \"timestamp\": \"2025-04-03T17:17:15.811+0530\", \"cleaned_body\": \"We understand your point, but I don\\u2019t think this qualifies as a bug. We will try to address this in later point of time, and if it is urgent now, please come via the enhancement route.\\n\\n you can check with tech if there are any ways this can be picked up.\", \"mentions\": [\"557058:916077b0-83f3-4697-ba71-ecabb879dd42\", \"712020:1a63b6be-da7c-4e02-bc73-bcbb119be206\"]}, {\"author\": \"Manish Mishra\", \"timestamp\": \"2025-04-23T11:03:36.192+0530\", \"cleaned_body\": \",\\n\\nthis ticket cannot be closed since system is not working as per expectations. We are not getting reasons for change in tier correctly. Whatever been the backend reason for this but this is bug for us, \\n\\nRe-opening the ticket. \\n\\n  - Please help us to prioritize this. \\n\\ncc\", \"mentions\": [\"557058:a46004b7-b075-468c-be4c-75e0177a4d78\", \"5af996675f0d5b06c28aa5d1\", \"61caa4ed7aa7ac0070c7baf9\", \"712020:4e0ad660-73a7-4f73-ab42-89c3d9b971f4\", \"712020:f75bf7f8-358c-490b-a7ed-821c033b0156\", \"5a93b684eb332d43753f7c4a\", \"6215fe7c37096d006a69169a\"]}, {\"author\": \"Suresh Moturi\", \"timestamp\": \"2025-04-23T11:04:49.866+0530\", \"cleaned_body\": \"Hi  ,\\n\\nThis was communicated as Enhancement from PM team as well, so how this can be a Bug ?\", \"mentions\": [\"557058:916077b0-83f3-4697-ba71-ecabb879dd42\"]}, {\"author\": \"Mayur Gupta\", \"timestamp\": \"2025-04-23T11:17:11.892+0530\", \"cleaned_body\": \"Upgrade and renew/downgrade are together considered as one flow of functionality in any Tiering use case.\\n\\nHow can a functionality i.e notes in this case for upgrade behave a certain way while it\\u2019s other counterpart i.e. downgrade behaves in a different way and be not classified as a production bug.\\n\\nIf this is the case, then are we indirectly asking customers to use only upgrades when you define tier use cases and we are not accountable for the same use case in renew/downgrade.\\n\\nThis can\\u2019t be treated as a product enhancement and we are not aligned with the suggestions shared below.\\n\\n Please set up a call to discuss this with the product/tech team.\\n\\nKindly don\\u2019t close this ticket until all of us are aligned on this.\\n\\ncc\", \"mentions\": [\"5af996675f0d5b06c28aa5d1\", \"63e36ef1c3eb74ad8e99682b\", \"712020:1a63b6be-da7c-4e02-bc73-bcbb119be206\", \"557058:a46004b7-b075-468c-be4c-75e0177a4d78\", \"557058:916077b0-83f3-4697-ba71-ecabb879dd42\", \"61caa4ed7aa7ac0070c7baf9\"]}, {\"author\": \"Pranav Handoo\", \"timestamp\": \"2025-04-23T11:32:26.097+0530\", \"cleaned_body\": \"scheduled a call at 12:30pm\", \"mentions\": [\"63e36ef1c3eb74ad8e99682b\", \"712020:1a63b6be-da7c-4e02-bc73-bcbb119be206\", \"712020:4e0ad660-73a7-4f73-ab42-89c3d9b971f4\", \"557058:916077b0-83f3-4697-ba71-ecabb879dd42\"]}, {\"author\": \"Karan Gehlot\", \"timestamp\": \"2025-04-23T11:50:00.247+0530\", \"cleaned_body\": \",\\n\\nPlease help us understand on why is this getting raised now ? Since TATA uses daily downgrade flow and this has been the behaviour always, why has it become a point of discussion now ?\", \"mentions\": [\"712020:4e0ad660-73a7-4f73-ab42-89c3d9b971f4\", \"712020:f75bf7f8-358c-490b-a7ed-821c033b0156\", \"557058:916077b0-83f3-4697-ba71-ecabb879dd42\"]}, {\"author\": \"Manish Mishra\", \"timestamp\": \"2025-04-23T12:07:41.898+0530\", \"cleaned_body\": \",\\n\\nNPR program launch in Feb-2025 and recently they have also introduced bonus tier function via Partner program Linking and delinking. Due to this they now trying to check the reason for system led actions. Earlier they were not worried about those reason since in TataNeu it was only one tier. \\n\\nTheir use case and utilization of reasons for tier change is increase now due to launch of NPR program which is 5 tiers now. \\n\\nI hope this is clear.\", \"mentions\": [\"557058:4da0f6db-16b9-4b3c-8061-d5e4317408a7\"]}, {\"author\": \"Manish Mishra\", \"timestamp\": \"2025-04-23T12:31:44.604+0530\", \"cleaned_body\": \", \\n\\nAdding to last comment, I have also observed that upgrade notes are also not correct. \\nEx - \\n\\nProgram - Titan (1000087)\\n\\nCustomer ID - 39747802\\n\\nTracker Value - 1164.831\\n\\n\\nCustomer upgraded with tracker value as per strategy but notes shows something else. \\n\\nScreenshot below - \\n\\n!image-20250423-070006.png|width=1331,height=438,alt=\\\"image-20250423-070006.png\\\"!\", \"mentions\": [\"557058:4da0f6db-16b9-4b3c-8061-d5e4317408a7\"]}, {\"author\": \"Manish Mishra\", \"timestamp\": \"2025-04-23T13:02:04.152+0530\", \"cleaned_body\": \"Hi  ,\\n\\nAs we discussed, here is the renewal example as well - \\nCustomer ID (userID) - 33911032\\n\\nTracker Value - 732.176 \\n\\nCustomer Get renew to Gold on 14-03-2025 \\n\\nNotes are showing as Spend and Visit while my strategy for renew is on Tracker Value. \\n\\n!image-20250423-073001.png|width=1244,height=406,alt=\\\"image-20250423-073001.png\\\"!\\n\\nConfig Screenshot - \\n\\n!image-20250423-073157.png|width=768,height=611,alt=\\\"image-20250423-073157.png\\\"!\", \"mentions\": [\"63e36ef1c3eb74ad8e99682b\"]}, {\"author\": \"Bhuvaneshwari Seshachalam\", \"timestamp\": \"2025-04-23T13:35:16.863+0530\", \"cleaned_body\": \"This is clearly a bug, not an enhancement since notes are coming up but incorrect. Please can you have someone revisit and sync with Manish to replicate the issue.\", \"mentions\": [\"557058:a46004b7-b075-468c-be4c-75e0177a4d78\"]}, {\"author\": \"Naresh Silla\", \"timestamp\": \"2025-04-23T13:36:14.891+0530\", \"cleaned_body\": \"there seems to be a mismatch between what the reason is mentioned in the note & what the actual reason is.\", \"mentions\": [\"557058:4da0f6db-16b9-4b3c-8061-d5e4317408a7\"]}, {\"author\": \"Suresh Moturi\", \"timestamp\": \"2025-04-23T13:43:49.343+0530\", \"cleaned_body\": \"Please check once on the latest raised concern\", \"mentions\": [\"712020:b7de7d8b-a82d-4628-817b-3433e570eb61\"]}, {\"author\": \"M Giridhar Kini\", \"timestamp\": \"2025-04-24T14:39:54.678+0530\", \"cleaned_body\": \"Hi  , as discussed and tested yesterday, we were able to replicate the upgrade note discrepancy. The same has also been seen in these sample cases in prod. Apart from this, as mentioned prior, the downgrade & renewal notes are being set via a predefined template, hence it does not satisfy the requirements. Please look into this further, and fix as required.\\n\\nLogs:\\n[Customer 1|https://grafana.tatacrm.cctools.capillarytech.com/tunnel/5ee20a9105ee817878209f5b/grafana/explore?orgId=1&left=%7B%22datasource%22:%22PDDCC14AE8A39CBD0%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22PDDCC14AE8A39CBD0%22%7D,%22editorMode%22:%22builder%22,%22expr%22:%22%7Bapp%3D%5C%22emf-write-a%5C%22%7D%20%7C%3D%20%6075d72e40753ad741b3a4f9ccde7ec208%60%22,%22queryType%22:%22range%22,%22maxLines%22:5000%7D%5D,%22range%22:%7B%22from%22:%221742288519777%22,%22to%22:%221742289418777%22%7D%7D]\\n[Customer 2|https://grafana.tatacrm.cctools.capillarytech.com/tunnel/5ee20a9105ee817878209f5b/grafana/explore?orgId=1&left=%7B%22datasource%22:%22PDDCC14AE8A39CBD0%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22PDDCC14AE8A39CBD0%22%7D,%22editorMode%22:%22builder%22,%22expr%22:%22%7Bapp%3D%5C%22emf-write-a%5C%22%7D%20%7C%3D%20%602caf7a2d7e06742c8a69a8ed483dd900%60%22,%22queryType%22:%22range%22,%22maxLines%22:5000%7D%5D,%22range%22:%7B%22from%22:%221742490300000%22,%22to%22:%221742490599000%22%7D%7D]\\n[Customer 3|https://grafana.tatacrm.cctools.capillarytech.com/tunnel/5ee20a9105ee817878209f5b/grafana/explore?orgId=1&left=%7B%22datasource%22:%22PDDCC14AE8A39CBD0%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22PDDCC14AE8A39CBD0%22%7D,%22editorMode%22:%22builder%22,%22expr%22:%22%7Bapp%3D%5C%22emf-write-a%5C%22%7D%20%7C%3D%20%6081ce46f618338bb653bc5e8e13eae06c%60%22,%22queryType%22:%22range%22,%22maxLines%22:5000%7D%5D,%22range%22:%7B%22from%22:%221742490300000%22,%22to%22:%221742490599000%22%7D%7D]\\n\\nCC\", \"mentions\": [\"632c414188ed2ebef979f26f\", \"557058:a46004b7-b075-468c-be4c-75e0177a4d78\", \"712020:44581a6d-de37-4173-98b5-1868d47e3a82\", \"557058:4da0f6db-16b9-4b3c-8061-d5e4317408a7\"]}, {\"author\": \"Ruchi Tetwal\", \"timestamp\": \"2025-05-06T17:00:01.506+0530\", \"cleaned_body\": \"We would like to confirm the notes pattern for the code changes required for slab renewal and downgrade. Below is a summary of the patterns:\\n\\n\\nRegular Tier Changes (Downgrade / Renewal)\\n\\n# Downgrade\\nPattern:\\n{{Tier downgraded from Slab[srcSlabNumber]: Name [srcSlabName] to Slab[targetSlabNumber]: Name [targetSlabName]}}\\n\\n If criteria filters exist:\\n{{Failed to maintain required criteria - [criteria details]}}\\n If no criteria filters:\\n{{Criteria: Automatic downgrade/renew as per schedule}}\\n Ends with:\\n{{(Downgraded on: [date])}}\\n\\nExample:\\n{{Tier downgraded from Slab[2]: Name [Silver] to Slab[1]: Name [Bronze] Failed to maintain required criteria - ExpressionRelation: [[purchase,numVisits]], Visits: required 5, achieved 3; Purchase: required 1000, achieved 800; (Downgraded on: 15-Mar-2024)}} \\n\\n\\n# Renewal\\nPattern:\\n{{Tier renewed for Slab[slabNumber]: Name [slabName]}}\\n\\n If criteria filters exist:\\n{{Succeeded to maintain required criteria - [criteria details]}}\\n If no criteria filters:\\n{{Criteria: Automatic downgrade/renew as per schedule}}\\n Ends with:\\n{{(Renewed on: [date])}}\\n\\nExample:\\n{{Tier renewed for Slab[2]: Name [Silver] Succeeded to maintain required criteria - Visits: required 5, achieved 6; (Renewed on: 15-Mar-2024)}}\\n\\n\\nCriteria Details (if applicable):\\n\\n Expression Relation (if present): {{ExpressionRelation: [relation]}}\\n Visits: {{Visits: required [numVisits], achieved [actualVisits]}}\\n Purchase: {{Purchase: required [amount], achieved [actualAmount]}}\\n Points: {{Points: required [points], achieved [actualPoints]}} (with point type details if applicable)\\n Trackers: {{Tracker [trackerId]: required [value], achieved [actualValue]}}\\n\\n\\nOther Tier Notes (Existing Patterns - As Designed):\\n\\n Import: {{Import Id #[importId] [notes from customerInfo]}}\\n Partner Program Tier Sync: {{Tier synced due to existing enrollment with partner program: [ppId]}}\\n\\n\\nPlease confirm if this note pattern are fine. Let us know if any modifications are needed.\\n\\nThanks!\\n\\ncc:\", \"mentions\": [\"63e36ef1c3eb74ad8e99682b\", \"712020:44581a6d-de37-4173-98b5-1868d47e3a82\", \"557058:4da0f6db-16b9-4b3c-8061-d5e4317408a7\", \"602a53dfddba8b007029a359\"]}, {\"author\": \"Naresh Silla\", \"timestamp\": \"2025-05-07T10:31:06.929+0530\", \"cleaned_body\": \"Looking good  . Just that, let\\u2019s try to mention the trackerName along with trackerID as we are not exposing the ID anywhere on the UI. Also,  would it not be possible to handle upgradeNotes in the same flow?\\n\\n do check the samples given above, and share your feedback as well.\\n\\ncc:\", \"mentions\": [\"63db67df724a5c79c7b84b53\", \"712020:44581a6d-de37-4173-98b5-1868d47e3a82\", \"557058:916077b0-83f3-4697-ba71-ecabb879dd42\", \"5af996675f0d5b06c28aa5d1\"]}, {\"author\": \"Manish Mishra\", \"timestamp\": \"2025-05-07T15:12:25.041+0530\", \"cleaned_body\": \"Hi   and  \\n\\nSome suggestions - \\n\\nIn Downgrade - \\nChange text - {{Failed to maintain required criteria}}\\n\\nTo -  Downgraded since \\u201crequirement not satisfied / fulfilled\\u201d {color:#ff5630}or{color} \\u201crequirement not maintained\\u201d \\n\\nIn Renewal - \\n\\nChange  -  {{Succeeded to maintain required criteria}}\\n\\nTo - Renewed since requirement is satisfied / fulfilled / maintained.\\n\\n\\n\\nAlso please confirm on the upgrade remark discrepancy. What we are planning to do for that fix. \\n\\ncc\", \"mentions\": [\"63e36ef1c3eb74ad8e99682b\", \"63db67df724a5c79c7b84b53\", \"712020:4e0ad660-73a7-4f73-ab42-89c3d9b971f4\", \"712020:f75bf7f8-358c-490b-a7ed-821c033b0156\"]}, {\"author\": \"Manish Mishra\", \"timestamp\": \"2025-05-07T17:26:26.404+0530\", \"cleaned_body\": \"Also for renew case - \\n\\nUser word Tier retained / Membership extended in place of Tier renewed.\", \"mentions\": []}, {\"author\": \"Pranav Handoo\", \"timestamp\": \"2025-05-09T14:58:50.929+0530\", \"cleaned_body\": \"Please confirm what will be ETA for the closure of this.\", \"mentions\": [\"63db67df724a5c79c7b84b53\", \"63e36ef1c3eb74ad8e99682b\"]}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            cleaned_comments  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      "1  We understand your point, but I don’t think this qualifies as a bug. We will try to address this in later point of time, and if it is urgent now, please come via the enhancement route.\\n\\n you can check with tech if there are any ways this can be picked up.\\n,\\n\\nthis ticket cannot be closed since system is not working as per expectations. We are not getting reasons for change in tier correctly. Whatever been the backend reason for this but this is bug for us, \\n\\nRe-opening the ticket. \\n\\n  - Please help us to prioritize this. \\n\\ncc\\nHi  ,\\n\\nThis was communicated as Enhancement from PM team as well, so how this can be a Bug ?\\nUpgrade and renew/downgrade are together considered as one flow of functionality in any Tiering use case.\\n\\nHow can a functionality i.e notes in this case for upgrade behave a certain way while it’s other counterpart i.e. downgrade behaves in a different way and be not classified as a production bug.\\n\\nIf this is the case, then are we indirectly asking customers to use only upgrades when you define tier use cases and we are not accountable for the same use case in renew/downgrade.\\n\\nThis can’t be treated as a product enhancement and we are not aligned with the suggestions shared below.\\n\\n Please set up a call to discuss this with the product/tech team.\\n\\nKindly don’t close this ticket until all of us are aligned on this.\\n\\ncc\\nThis is clearly a bug, not an enhancement since notes are coming up but incorrect. Please can you have someone revisit and sync with Manish to replicate the issue.  \n",
      "\n",
      "Missing values in 'cleaned_comments': 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# --- MODIFIED Step 1 & 2 for 'comments' (Filter by Problem Keywords, Text Only, Sorted) ---\n",
    "\n",
    "comments_col = 'comments' # Ensure this matches your DataFrame column name\n",
    "PROBLEM_KEYWORDS = [\n",
    "    \"error\", \"fail\", \"not working\", \"unable to\", \"crash\", \"timeout\",\n",
    "    \"unexpected\", \"broken\", \"incorrect\", \"missing\", \"issue\", \"bug\",\n",
    "    \"problem\", \"exception\", \"doesn't work\", \"can't\", \"cannot\" # Added a few more common ones\n",
    "]\n",
    "# Create a regex pattern for these keywords, case-insensitive, matching whole words\n",
    "# \\b ensures we match \"error\" and not \"terrorist\"\n",
    "problem_keyword_pattern = r'\\b(?:' + '|'.join(re.escape(k) for k in PROBLEM_KEYWORDS) + r')\\b'\n",
    "\n",
    "def parse_filter_and_format_comments(comments_json_str): # Renamed for clarity\n",
    "    \"\"\"\n",
    "    Parses a JSON string of comments, sorts them by timestamp,\n",
    "    FILTERS them for problem_keywords in 'cleaned_body',\n",
    "    extracts ONLY the 'cleaned_body' of filtered comments, and joins them.\n",
    "    \"\"\"\n",
    "    if pd.isna(comments_json_str) or not comments_json_str.strip():\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        comments_list = json.loads(comments_json_str)\n",
    "        if not isinstance(comments_list, list) or not comments_list:\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            comments_list.sort(key=lambda x: x.get('timestamp', ''))\n",
    "        except TypeError:\n",
    "            pass # Proceed with original order if sorting fails\n",
    "\n",
    "        filtered_comment_texts = []\n",
    "        for comment_obj in comments_list:\n",
    "            comment_text = comment_obj.get('cleaned_body', '')\n",
    "            if comment_text:\n",
    "                # Check if any problem keyword is in the comment_text (case-insensitive)\n",
    "                if re.search(problem_keyword_pattern, comment_text, flags=re.IGNORECASE):\n",
    "                    filtered_comment_texts.append(comment_text)\n",
    "        \n",
    "        return \"\\n\".join(filtered_comment_texts) # Join only the filtered comment texts\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        # print(f\"Error in parse_filter_and_format_comments: {e}\") # For debugging\n",
    "        return \"\"\n",
    "\n",
    "# --- Define ALL your other cleaning functions here (or ensure they are in executed cells above) ---\n",
    "# strip_jira_markup (ensure it removes \"Original Author:\", \"Posted on:\")\n",
    "# normalize_whitespace\n",
    "# standardize_case\n",
    "# remove_user_mentions\n",
    "# remove_urls\n",
    "# manage_punctuation\n",
    "# process_code_and_stack_traces\n",
    "# remove_id_data_blobs (your Step 9.5)\n",
    "# remove_or_replace_numbers (your Step 9.7)\n",
    "# ...and any other functions like remove_domain_specific_data if you developed it further.\n",
    "\n",
    "\n",
    "# --- RE-RUN THE ENTIRE CLEANING PIPELINE for 'cleaned_comments' ---\n",
    "# Starting from the NEW parsing and filtering.\n",
    "\n",
    "print(\"Re-processing 'cleaned_comments' with KEYWORD FILTERING and all subsequent cleaning...\")\n",
    "\n",
    "# 1. Apply the NEW parsing and filtering function\n",
    "df['cleaned_comments'] = df[comments_col].apply(parse_filter_and_format_comments)\n",
    "print(\"Step 1 (parse_filter_and_format_comments) for comments: Done\")\n",
    "\n",
    "# Display the first few rows of original and cleaned comments\n",
    "print(\"\\nOriginal and cleaned 'comments' (with timestamp sorting):\")\n",
    "print(df[[comments_col, 'cleaned_comments']].head(2))\n",
    "\n",
    "print(f\"\\nMissing values in 'cleaned_comments': {df['cleaned_comments'].isnull().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# (Re-define the function if you're in a new cell, or modify the existing cell)\n",
    "def strip_jira_markup(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # {panel:...}content{panel} -> content\n",
    "    text = re.sub(r'\\{panel:[^}]*}(.*?)\\{panel}', r'\\1', text, flags=re.DOTALL)\n",
    "    \n",
    "    # {code[:lang]}content{code} -> content\n",
    "    text = re.sub(r'\\{code:[^}]*}(.*?)\\{code}', r'\\1', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\{code}(.*?)\\{code}', r'\\1', text, flags=re.DOTALL)\n",
    "\n",
    "    # {color:...}text{color} -> text\n",
    "    text = re.sub(r'\\{color:[^}]*}(.*?)\\{color}', r'\\1', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove \"Posted on: ...\" lines (potentially left over from panel content)\n",
    "    # This regex matches lines that start with \"Posted on:\" possibly with leading/trailing whitespace on the line itself.\n",
    "    text = re.sub(r'^\\s*Posted on:.*?\\n', '', text, flags=re.MULTILINE)\n",
    "    # If it might not end with a newline (e.g., last line of comment):\n",
    "    text = re.sub(r'^\\s*Posted on:.*?$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "    # Optional: Remove \"Original Author: ...\" lines if desired\n",
    "    # text = re.sub(r'^\\s*Original Author:.*?\\n', '', text, flags=re.MULTILINE)\n",
    "    # text = re.sub(r'^\\s*Original Author:.*?$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # h1. to h6. headers\n",
    "    text = re.sub(r'h[1-6]\\.\\s*', '', text)\n",
    "    # *bold*\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    # _italic_\n",
    "    text = re.sub(r'_(.*?)_', r'\\1', text)\n",
    "    # +underline+\n",
    "    text = re.sub(r'\\+(.*?)\\+', r'\\1', text)\n",
    "    # -strikethrough-\n",
    "    text = re.sub(r'-(.*?)-', r'\\1', text)\n",
    "    # ??citation??\n",
    "    text = re.sub(r'\\?\\?(.*?)\\?\\?', r'\\1', text)\n",
    "    # {{monospaced}}\n",
    "    text = re.sub(r'\\{\\{(.*?)\\}\\}', r'\\1', text)\n",
    "    # bq. blockquote\n",
    "    text = re.sub(r'bq\\.\\s+', '', text)\n",
    "    # Links\n",
    "    text = re.sub(r'\\[([^|\\]]+)\\|[^\\]]+\\]', r'\\1', text)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]', r'\\1', text)\n",
    "    # Images\n",
    "    text = re.sub(r'!([^!]+)!', '', text)\n",
    "    # Basic list markers\n",
    "    text = re.sub(r'^\\s*[\\*#-]\\s+', '', text, flags=re.MULTILINE)\n",
    "    # noformat, quote\n",
    "    text = re.sub(r'\\{noformat\\}(.*?)\\{noformat\\}', r'\\1', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\{quote\\}(.*?)\\{quote\\}', r'\\1', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- You would then re-apply this updated function ---\n",
    "# --- and the subsequent whitespace and case normalization steps ---\n",
    "\n",
    "# print(\"Re-applying cleaning starting from Step 3 (Markup Removal)...\")\n",
    "\n",
    "# Step 3: Apply updated strip_jira_markup\n",
    "# df['cleaned_summary'] = df[summary_col].fillna('').astype(str).apply(strip_jira_markup) # Re-start from original filled NAs if making big changes\n",
    "# df['cleaned_description'] = df[description_col].fillna('').astype(str).apply(strip_jira_markup)\n",
    "# df['cleaned_comments'] = df[comments_col].apply(parse_and_format_comments).apply(strip_jira_markup) # Re-parse comments then strip\n",
    "\n",
    "# For simplicity if previous steps (1 & 2) are solid, you can just apply to the already partially cleaned columns:\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(strip_jira_markup)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(strip_jira_markup)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(strip_jira_markup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after whitespace normalization (first 2 rows):\n",
      "Cleaned Summary:\n",
      "0                                                                           EMFDEV Add Coupon benefit\n",
      "1    Tata Digital Prod (1000006): Tier Downgrade Job saves wrong Note for tier renew/downgrade reason\n",
      "\n",
      "Cleaned Description:\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "1    Hi Team, We have received request from TATA to explain on how to know if customer is getting renewed or downgraded for which reason. We know that for Upgrades via system, we get proper Notes added as reason for upgrade. This has been verified and comments are accurate. While validating same for Renew and Downgrade cases, we observed that notes are having different value but customer get renew or downgrade for different reason. Impacted Data Sample - Export Job ID - https://tata.intouch.capillarytech.com/analytics/v2/exports/schedules/8625/view Attached exported Data - ^TierChangelogsampleupdated02.zip Program : Titan Upgrade / Downgrade - Based on Loyalty points earned in last 2 years. (Points tracker used) Sample Records Here customer Upgrade / downgrade / Renew happened via Strategy based on Points tracker. For upgrade cases comments are correct but for renew/Downgrade cases the reason (comment) is not correct. We need correct reason for actions Renew / Downgrade based on which the action was taken. |Datedate|Timetime|customerslabserialno|customerslabslabname|PreviousCustomerSlabserialno|PreviousCustomerSlabslabname|SlabUpgradeEventTypeUpgradeEventName|SlabUpgradeEventTypeUpgradeEventCategory|latestupdateddatedate|latestupdatedtimetime|Usercustomerexternalid|EventProgramprogramname|EventProgramProgramId|slabchangeactionslabchangeaction|slabchangesourceslabchangesource|Notes|User_Id|Id| |17032025|01:15:00|1|Silver|2|Gold|CustomerRegistration|CustomerRegistration|17032025|01:15:00|dd3ddb0bb5c22ba65ade5d3f2a55e576|Titan|1000087|DOWNGRADE|STRATEGY|pointsenginetier_downgrade visits: 0 purchase: 0.000|6535282|489106370| |14032025|01:15:00|2|Gold|2|Gold|CustomerRegistration|CustomerRegistration|14032025|01:16:00|40e24b06415ace8e44b3f734171c67dc|Titan|1000087|RENEWAL|STRATEGY|pointsenginetier_downgrade visits: 2 purchase: 192138.62|33911032|489047601| |17032025|01:15:00|1|Silver|2|Gold|CustomerRegistration|CustomerRegistration|17032025|01:15:00|5ba624e0df6032e23d6d4c6bc06f54ca|Titan|1000087|DOWNGRADE|STRATEGY|pointsenginetier_downgrade visits: 2 purchase: 6085.000|37668590|489106462| |18032025|14:46:00|2|Gold|1|Silver|INVALID|INVALID|18032025|14:46:00|6a74b7b6fd832e8c6a07fbed9fed7ec6|Titan|1000087|UPGRADE|STRATEGY|Upgrading to Slab : Slab2: Name Gold Description Slab 2 , Criteria: Primary, Tracker Id: 1326, Tracker Value: 0|39747802|489118372| Export Job and File is attached. Same reason also shows in membercare.\n",
      "\n",
      "Cleaned Comments (example from first ticket with comments):\n",
      "(Ticket index 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Step 4: Normalize Whitespace ---\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 4.1: Replace multiple spaces, tabs, and newlines with a single space\n",
    "    # \\s+ matches one or more whitespace characters (space, tab, newline, etc.)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Step 4.2: Trim leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply to the relevant text columns\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(normalize_whitespace)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(normalize_whitespace)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(normalize_whitespace)\n",
    "\n",
    "print(\"\\nText after whitespace normalization (first 2 rows):\")\n",
    "print(\"Cleaned Summary:\")\n",
    "print(df['cleaned_summary'].head(2).to_string())\n",
    "print(\"\\nCleaned Description:\")\n",
    "print(df['cleaned_description'].head(2).to_string())\n",
    "print(\"\\nCleaned Comments (example from first ticket with comments):\")\n",
    "\n",
    "first_comment_idx = df[df[comments_col].fillna('').str.len() > 0].index.min()\n",
    "if pd.notna(first_comment_idx):\n",
    "    print(f\"(Ticket index {first_comment_idx})\")\n",
    "    # To see the effect clearly, especially if there were many newlines in comments,\n",
    "    # print the version before this step vs after, if you have it saved.\n",
    "    # For now, just printing the current state.\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    print(\"No comments found to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after case standardization (first 2 rows):\n",
      "Cleaned Summary:\n",
      "0                                                                           emfdev add coupon benefit\n",
      "1    tata digital prod (1000006): tier downgrade job saves wrong note for tier renew/downgrade reason\n",
      "\n",
      "Cleaned Description:\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "1    hi team, we have received request from tata to explain on how to know if customer is getting renewed or downgraded for which reason. we know that for upgrades via system, we get proper notes added as reason for upgrade. this has been verified and comments are accurate. while validating same for renew and downgrade cases, we observed that notes are having different value but customer get renew or downgrade for different reason. impacted data sample - export job id - https://tata.intouch.capillarytech.com/analytics/v2/exports/schedules/8625/view attached exported data - ^tierchangelogsampleupdated02.zip program : titan upgrade / downgrade - based on loyalty points earned in last 2 years. (points tracker used) sample records here customer upgrade / downgrade / renew happened via strategy based on points tracker. for upgrade cases comments are correct but for renew/downgrade cases the reason (comment) is not correct. we need correct reason for actions renew / downgrade based on which the action was taken. |datedate|timetime|customerslabserialno|customerslabslabname|previouscustomerslabserialno|previouscustomerslabslabname|slabupgradeeventtypeupgradeeventname|slabupgradeeventtypeupgradeeventcategory|latestupdateddatedate|latestupdatedtimetime|usercustomerexternalid|eventprogramprogramname|eventprogramprogramid|slabchangeactionslabchangeaction|slabchangesourceslabchangesource|notes|user_id|id| |17032025|01:15:00|1|silver|2|gold|customerregistration|customerregistration|17032025|01:15:00|dd3ddb0bb5c22ba65ade5d3f2a55e576|titan|1000087|downgrade|strategy|pointsenginetier_downgrade visits: 0 purchase: 0.000|6535282|489106370| |14032025|01:15:00|2|gold|2|gold|customerregistration|customerregistration|14032025|01:16:00|40e24b06415ace8e44b3f734171c67dc|titan|1000087|renewal|strategy|pointsenginetier_downgrade visits: 2 purchase: 192138.62|33911032|489047601| |17032025|01:15:00|1|silver|2|gold|customerregistration|customerregistration|17032025|01:15:00|5ba624e0df6032e23d6d4c6bc06f54ca|titan|1000087|downgrade|strategy|pointsenginetier_downgrade visits: 2 purchase: 6085.000|37668590|489106462| |18032025|14:46:00|2|gold|1|silver|invalid|invalid|18032025|14:46:00|6a74b7b6fd832e8c6a07fbed9fed7ec6|titan|1000087|upgrade|strategy|upgrading to slab : slab2: name gold description slab 2 , criteria: primary, tracker id: 1326, tracker value: 0|39747802|489118372| export job and file is attached. same reason also shows in membercare.\n",
      "\n",
      "Cleaned Comments (example from first ticket with comments):\n",
      "(Ticket index 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Standardize Case ---\n",
    "\n",
    "def standardize_case(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 5.1: Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply to the relevant text columns\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(standardize_case)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(standardize_case)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(standardize_case)\n",
    "\n",
    "print(\"\\nText after case standardization (first 2 rows):\")\n",
    "print(\"Cleaned Summary:\")\n",
    "print(df['cleaned_summary'].head(2).to_string())\n",
    "print(\"\\nCleaned Description:\")\n",
    "print(df['cleaned_description'].head(2).to_string())\n",
    "print(\"\\nCleaned Comments (example from first ticket with comments):\")\n",
    "\n",
    "# Assuming 'comments_col' variable is still defined from previous steps\n",
    "first_comment_idx = df[df[comments_col].fillna('').str.len() > 0].index.min()\n",
    "if pd.notna(first_comment_idx):\n",
    "    print(f\"(Ticket index {first_comment_idx})\")\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    print(\"No comments found to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['cleaned_summary', 'cleaned_description', 'cleaned_comments']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after removing user mentions (first 2 rows):\n",
      "Cleaned Summary:\n",
      "0                                                                           emfdev add coupon benefit\n",
      "1    tata digital prod (1000006): tier downgrade job saves wrong note for tier renew/downgrade reason\n",
      "\n",
      "Cleaned Description:\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "1    hi team, we have received request from tata to explain on how to know if customer is getting renewed or downgraded for which reason. we know that for upgrades via system, we get proper notes added as reason for upgrade. this has been verified and comments are accurate. while validating same for renew and downgrade cases, we observed that notes are having different value but customer get renew or downgrade for different reason. impacted data sample - export job id - https://tata.intouch.capillarytech.com/analytics/v2/exports/schedules/8625/view attached exported data - ^tierchangelogsampleupdated02.zip program : titan upgrade / downgrade - based on loyalty points earned in last 2 years. (points tracker used) sample records here customer upgrade / downgrade / renew happened via strategy based on points tracker. for upgrade cases comments are correct but for renew/downgrade cases the reason (comment) is not correct. we need correct reason for actions renew / downgrade based on which the action was taken. |datedate|timetime|customerslabserialno|customerslabslabname|previouscustomerslabserialno|previouscustomerslabslabname|slabupgradeeventtypeupgradeeventname|slabupgradeeventtypeupgradeeventcategory|latestupdateddatedate|latestupdatedtimetime|usercustomerexternalid|eventprogramprogramname|eventprogramprogramid|slabchangeactionslabchangeaction|slabchangesourceslabchangesource|notes|user_id|id| |17032025|01:15:00|1|silver|2|gold|customerregistration|customerregistration|17032025|01:15:00|dd3ddb0bb5c22ba65ade5d3f2a55e576|titan|1000087|downgrade|strategy|pointsenginetier_downgrade visits: 0 purchase: 0.000|6535282|489106370| |14032025|01:15:00|2|gold|2|gold|customerregistration|customerregistration|14032025|01:16:00|40e24b06415ace8e44b3f734171c67dc|titan|1000087|renewal|strategy|pointsenginetier_downgrade visits: 2 purchase: 192138.62|33911032|489047601| |17032025|01:15:00|1|silver|2|gold|customerregistration|customerregistration|17032025|01:15:00|5ba624e0df6032e23d6d4c6bc06f54ca|titan|1000087|downgrade|strategy|pointsenginetier_downgrade visits: 2 purchase: 6085.000|37668590|489106462| |18032025|14:46:00|2|gold|1|silver|invalid|invalid|18032025|14:46:00|6a74b7b6fd832e8c6a07fbed9fed7ec6|titan|1000087|upgrade|strategy|upgrading to slab : slab2: name gold description slab 2 , criteria: primary, tracker id: 1326, tracker value: 0|39747802|489118372| export job and file is attached. same reason also shows in membercare.\n",
      "\n",
      "Cleaned Comments (example from first ticket with comments):\n",
      "(Ticket index 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Step 6: Handle User Mentions and Tags ---\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove Jira specific mentions like [~username] or [~accountid:xxxx...]\n",
    "    text = re.sub(r'\\[~[^\\]]+\\]', '', text)\n",
    "    \n",
    "    # Remove common @username mentions\n",
    "    # This regex looks for @ followed by a sequence of word characters (letters, numbers, underscore)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply to the relevant text columns\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(remove_user_mentions)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(remove_user_mentions)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(remove_user_mentions)\n",
    "\n",
    "print(\"\\nText after removing user mentions (first 2 rows):\")\n",
    "print(\"Cleaned Summary:\")\n",
    "print(df['cleaned_summary'].head(2).to_string()) # Assuming pandas display options are set\n",
    "print(\"\\nCleaned Description:\")\n",
    "print(df['cleaned_description'].head(2).to_string())\n",
    "print(\"\\nCleaned Comments (example from first ticket with comments):\")\n",
    "\n",
    "# Assuming 'comments_col' and 'first_comment_idx' are defined from previous steps\n",
    "if 'first_comment_idx' in locals() and pd.notna(first_comment_idx) and first_comment_idx in df.index:\n",
    "    print(f\"(Ticket index {first_comment_idx})\")\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    # Fallback if first_comment_idx is not valid or not found\n",
    "    temp_idx = df[df[comments_col].fillna('').str.len() > 0].index.min()\n",
    "    if pd.notna(temp_idx):\n",
    "        print(f\"(Ticket index {temp_idx})\")\n",
    "        print(df.loc[temp_idx, 'cleaned_comments'])\n",
    "    else:\n",
    "        print(\"No comments found to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after removing URLs (first 2 rows):\n",
      "Cleaned Summary:\n",
      "0                                                                           emfdev add coupon benefit\n",
      "1    tata digital prod (1000006): tier downgrade job saves wrong note for tier renew/downgrade reason\n",
      "\n",
      "Cleaned Description:\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "1    hi team, we have received request from tata to explain on how to know if customer is getting renewed or downgraded for which reason. we know that for upgrades via system, we get proper notes added as reason for upgrade. this has been verified and comments are accurate. while validating same for renew and downgrade cases, we observed that notes are having different value but customer get renew or downgrade for different reason. impacted data sample - export job id -  attached exported data - ^tierchangelogsampleupdated02.zip program : titan upgrade / downgrade - based on loyalty points earned in last 2 years. (points tracker used) sample records here customer upgrade / downgrade / renew happened via strategy based on points tracker. for upgrade cases comments are correct but for renew/downgrade cases the reason (comment) is not correct. we need correct reason for actions renew / downgrade based on which the action was taken. |datedate|timetime|customerslabserialno|customerslabslabname|previouscustomerslabserialno|previouscustomerslabslabname|slabupgradeeventtypeupgradeeventname|slabupgradeeventtypeupgradeeventcategory|latestupdateddatedate|latestupdatedtimetime|usercustomerexternalid|eventprogramprogramname|eventprogramprogramid|slabchangeactionslabchangeaction|slabchangesourceslabchangesource|notes|user_id|id| |17032025|01:15:00|1|silver|2|gold|customerregistration|customerregistration|17032025|01:15:00|dd3ddb0bb5c22ba65ade5d3f2a55e576|titan|1000087|downgrade|strategy|pointsenginetier_downgrade visits: 0 purchase: 0.000|6535282|489106370| |14032025|01:15:00|2|gold|2|gold|customerregistration|customerregistration|14032025|01:16:00|40e24b06415ace8e44b3f734171c67dc|titan|1000087|renewal|strategy|pointsenginetier_downgrade visits: 2 purchase: 192138.62|33911032|489047601| |17032025|01:15:00|1|silver|2|gold|customerregistration|customerregistration|17032025|01:15:00|5ba624e0df6032e23d6d4c6bc06f54ca|titan|1000087|downgrade|strategy|pointsenginetier_downgrade visits: 2 purchase: 6085.000|37668590|489106462| |18032025|14:46:00|2|gold|1|silver|invalid|invalid|18032025|14:46:00|6a74b7b6fd832e8c6a07fbed9fed7ec6|titan|1000087|upgrade|strategy|upgrading to slab : slab2: name gold description slab 2 , criteria: primary, tracker id: 1326, tracker value: 0|39747802|489118372| export job and file is attached. same reason also shows in membercare.\n",
      "\n",
      "Cleaned Comments (example from first ticket with comments):\n",
      "(Ticket index 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls_robust(text): # Renamed to indicate it's an updated version\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    url_pattern = r\"\"\"\\b(?:(?:https?|ftp)://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])\"\"\"\n",
    "    \n",
    "    text = re.sub(url_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply to the relevant text columns\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(remove_urls)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(remove_urls)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(remove_urls)\n",
    "\n",
    "print(\"\\nText after removing URLs (first 2 rows):\")\n",
    "print(\"Cleaned Summary:\")\n",
    "print(df['cleaned_summary'].head(2).to_string()) # Assuming pandas display options are set\n",
    "print(\"\\nCleaned Description:\")\n",
    "print(df['cleaned_description'].head(2).to_string())\n",
    "print(\"\\nCleaned Comments (example from first ticket with comments):\")\n",
    "\n",
    "# Assuming 'comments_col' and 'first_comment_idx' are defined from previous steps\n",
    "# Make sure first_comment_idx is valid and exists in the DataFrame index\n",
    "if 'first_comment_idx' in locals() and pd.notna(first_comment_idx) and first_comment_idx in df.index:\n",
    "    print(f\"(Ticket index {first_comment_idx})\")\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    # Fallback to find any comment if first_comment_idx is not set or invalid\n",
    "    temp_idx = df[df[comments_col].fillna('').str.len() > 0].index.min()\n",
    "    if pd.notna(temp_idx):\n",
    "        print(f\"(Ticket index {temp_idx})\") # Use the newly found index\n",
    "        print(df.loc[temp_idx, 'cleaned_comments'])\n",
    "    else:\n",
    "        print(\"No comments found to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after managing punctuation (first 2 rows):\n",
      "Cleaned Summary:\n",
      "0                                                                        emfdev add coupon benefit\n",
      "1    tata digital prod 1000006 tier downgrade job saves wrong note for tier renew downgrade reason\n",
      "\n",
      "Cleaned Description:\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "1    hi team we have received request from tata to explain on how to know if customer is getting renewed or downgraded for which reason. we know that for upgrades via system we get proper notes added as reason for upgrade. this has been verified and comments are accurate. while validating same for renew and downgrade cases we observed that notes are having different value but customer get renew or downgrade for different reason. impacted data sample - export job id - attached exported data - tierchangelogsampleupdated02.zip program titan upgrade downgrade - based on loyalty points earned in last 2 years. points tracker used sample records here customer upgrade downgrade renew happened via strategy based on points tracker. for upgrade cases comments are correct but for renew downgrade cases the reason comment is not correct. we need correct reason for actions renew downgrade based on which the action was taken. datedate timetime customerslabserialno customerslabslabname previouscustomerslabserialno previouscustomerslabslabname slabupgradeeventtypeupgradeeventname slabupgradeeventtypeupgradeeventcategory latestupdateddatedate latestupdatedtimetime usercustomerexternalid eventprogramprogramname eventprogramprogramid slabchangeactionslabchangeaction slabchangesourceslabchangesource notes user_id id 17032025 01 15 00 1 silver 2 gold customerregistration customerregistration 17032025 01 15 00 dd3ddb0bb5c22ba65ade5d3f2a55e576 titan 1000087 downgrade strategy pointsenginetier_downgrade visits 0 purchase 0.000 6535282 489106370 14032025 01 15 00 2 gold 2 gold customerregistration customerregistration 14032025 01 16 00 40e24b06415ace8e44b3f734171c67dc titan 1000087 renewal strategy pointsenginetier_downgrade visits 2 purchase 192138.62 33911032 489047601 17032025 01 15 00 1 silver 2 gold customerregistration customerregistration 17032025 01 15 00 5ba624e0df6032e23d6d4c6bc06f54ca titan 1000087 downgrade strategy pointsenginetier_downgrade visits 2 purchase 6085.000 37668590 489106462 18032025 14 46 00 2 gold 1 silver invalid invalid 18032025 14 46 00 6a74b7b6fd832e8c6a07fbed9fed7ec6 titan 1000087 upgrade strategy upgrading to slab slab2 name gold description slab 2 criteria primary tracker id 1326 tracker value 0 39747802 489118372 export job and file is attached. same reason also shows in membercare.\n",
      "\n",
      "Cleaned Comments (example from first ticket with comments):\n",
      "(Ticket index 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string # To help identify punctuation, though we'll define our keep list\n",
    "\n",
    "# --- Step 8: Manage Special Characters and Punctuation ---\n",
    "\n",
    "def manage_punctuation(text, keep_punctuation=\".-_\"):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    processed_chars = []\n",
    "    for char in text:\n",
    "        # Keep alphanumeric characters, characters in our 'keep_punctuation' list,\n",
    "        # and existing whitespace (which will be normalized later).\n",
    "        if char.isalnum() or char in keep_punctuation or char.isspace():\n",
    "            processed_chars.append(char)\n",
    "        else:\n",
    "            # Replace other punctuation/special characters with a space\n",
    "            processed_chars.append(' ')\n",
    "            \n",
    "    text = \"\".join(processed_chars)\n",
    "    \n",
    "    # Normalize whitespace again, as replacing punctuation with spaces can create multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "    return text\n",
    "\n",
    "# Apply to the relevant text columns\n",
    "# You can change the keep_punctuation argument if needed, e.g., df['cleaned_summary'].apply(manage_punctuation, keep_punctuation=\".-_!?\")\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(manage_punctuation)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(manage_punctuation)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(manage_punctuation)\n",
    "\n",
    "print(\"\\nText after managing punctuation (first 2 rows):\")\n",
    "print(\"Cleaned Summary:\")\n",
    "print(df['cleaned_summary'].head(2).to_string())\n",
    "print(\"\\nCleaned Description:\")\n",
    "print(df['cleaned_description'].head(2).to_string())\n",
    "print(\"\\nCleaned Comments (example from first ticket with comments):\")\n",
    "\n",
    "# Assuming 'comments_col' and 'first_comment_idx' are defined from previous steps\n",
    "if 'first_comment_idx' in locals() and pd.notna(first_comment_idx) and first_comment_idx in df.index:\n",
    "    print(f\"(Ticket index {first_comment_idx})\")\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    temp_idx = df[df[comments_col].fillna('').str.len() > 0].index.min()\n",
    "    if pd.notna(temp_idx):\n",
    "        print(f\"(Ticket index {temp_idx})\")\n",
    "        print(df.loc[temp_idx, 'cleaned_comments'])\n",
    "    else:\n",
    "        print(\"No comments found to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after processing code/stack traces (first 2 rows):\n",
      "Cleaned Summary:\n",
      "0                                                                        emfdev add coupon benefit\n",
      "1    tata digital prod 1000006 tier downgrade job saves wrong note for tier renew downgrade reason\n",
      "\n",
      "Cleaned Description:\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "1    hi team we have received request from tata to explain on how to know if customer is getting renewed or downgraded for which reason. we know that for upgrades via system we get proper notes added as reason for upgrade. this has been verified and comments are accurate. while validating same for renew and downgrade cases we observed that notes are having different value but customer get renew or downgrade for different reason. impacted data sample - export job id - attached exported data - tierchangelogsampleupdated02.zip program titan upgrade downgrade - based on loyalty points earned in last 2 years. points tracker used sample records here customer upgrade downgrade renew happened via strategy based on points tracker. for upgrade cases comments are correct but for renew downgrade cases the reason comment is not correct. we need correct reason for actions renew downgrade based on which the action was taken. datedate timetime customerslabserialno customerslabslabname previouscustomerslabserialno previouscustomerslabslabname slabupgradeeventtypeupgradeeventname slabupgradeeventtypeupgradeeventcategory latestupdateddatedate latestupdatedtimetime usercustomerexternalid eventprogramprogramname eventprogramprogramid slabchangeactionslabchangeaction slabchangesourceslabchangesource notes user_id id 17032025 01 15 00 1 silver 2 gold customerregistration customerregistration 17032025 01 15 00 dd3ddb0bb5c22ba65ade5d3f2a55e576 titan 1000087 downgrade strategy pointsenginetier_downgrade visits 0 purchase 0.000 6535282 489106370 14032025 01 15 00 2 gold 2 gold customerregistration customerregistration 14032025 01 16 00 40e24b06415ace8e44b3f734171c67dc titan 1000087 renewal strategy pointsenginetier_downgrade visits 2 purchase 192138.62 33911032 489047601 17032025 01 15 00 1 silver 2 gold customerregistration customerregistration 17032025 01 15 00 5ba624e0df6032e23d6d4c6bc06f54ca titan 1000087 downgrade strategy pointsenginetier_downgrade visits 2 purchase 6085.000 37668590 489106462 18032025 14 46 00 2 gold 1 silver invalid invalid 18032025 14 46 00 6a74b7b6fd832e8c6a07fbed9fed7ec6 titan 1000087 upgrade strategy upgrading to slab slab2 name gold description slab 2 criteria primary tracker id 1326 tracker value 0 39747802 489118372 export job and file is attached. same reason also shows in membercare.\n",
      "\n",
      "Cleaned Comments (example from first ticket with comments):\n",
      "(Ticket index 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Step 9: Isolate or Neutralize Code Blocks and Stack Traces ---\n",
    "\n",
    "def process_code_and_stack_traces(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Identify and replace common stack trace patterns (Java, Python, C#, etc.)\n",
    "    # Regex for typical Java/C# stack trace lines: \"at package.class.method(File:line)\"\n",
    "    # or \"at package.class.method(Native Method)\"\n",
    "    # This is a simplified regex and might need refinement for more complex traces.\n",
    "    # It looks for multiple lines starting with \"at \" or common exception headers.\n",
    "    stack_trace_pattern = r'((?:[a-zA-Z0-9_]+\\.)+[a-zA-Z0-9_]+Exception(?:[:\\s].*)?\\n(?:^\\s*at .*(?:\\n|$))+)'\n",
    "    # Python's \"Traceback (most recent call last):\"\n",
    "    python_traceback_pattern = r'(Traceback \\(most recent call last\\):\\n(?:(?:^\\s*File \".*?\", line \\d+, in .*\\n)|(?:^\\s*.*\\n))*?\\w*Error:.*)'\n",
    "    \n",
    "    text = re.sub(stack_trace_pattern, ' <STACK_TRACE> ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(python_traceback_pattern, ' <STACK_TRACE> ', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 2. Identify and replace very specific code prompts/patterns\n",
    "    # Example: mysql> ... ; (simple version, assumes single line for now)\n",
    "    # This is highly heuristic.\n",
    "    # Matches \"mysql>\" followed by any characters non-greedily (.*?) until a semicolon.\n",
    "    # This is a basic example; real SQL can be multi-line and complex.\n",
    "    mysql_pattern = r'mysql>.*?;'\n",
    "    text = re.sub(mysql_pattern, ' <CODE_SNIPPET> ', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Example: content that looks like a block of code with many special chars / indentation\n",
    "    # This is harder. A placeholder for a more complex heuristic if needed later:\n",
    "    # If we find multiple lines with high density of { } ; ( ) or heavy indentation:\n",
    "    # multi_line_code_pattern = r'(?:^\\s*[\\w\\s]*[(){};][\\w\\s()\\[\\]{};:,.\"'\\'']*\\n){3,}' # Example: 3+ lines with code-like chars\n",
    "    # text = re.sub(multi_line_code_pattern, ' <CODE_SNIPPET> ', text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "    # The content from original Jira {code} blocks is already part of the 'text' here.\n",
    "    # If that content strongly resembles a stack trace or our simple SQL pattern, it will be replaced.\n",
    "    # Otherwise, it remains. A more robust way for {code} blocks would have been to replace\n",
    "    # them entirely with <CODE_SNIPPET> in the strip_jira_markup function itself if that's desired.\n",
    "\n",
    "    # Normalize whitespace again, as replacements can introduce or leave extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "    return text\n",
    "\n",
    "# Apply to the relevant text columns\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(process_code_and_stack_traces)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(process_code_and_stack_traces)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(process_code_and_stack_traces)\n",
    "\n",
    "print(\"\\nText after processing code/stack traces (first 2 rows):\")\n",
    "print(\"Cleaned Summary:\")\n",
    "print(df['cleaned_summary'].head(2).to_string())\n",
    "print(\"\\nCleaned Description:\")\n",
    "print(df['cleaned_description'].head(2).to_string())\n",
    "print(\"\\nCleaned Comments (example from first ticket with comments):\")\n",
    "\n",
    "# Assuming 'comments_col' and 'first_comment_idx' are defined\n",
    "if 'first_comment_idx' in locals() and pd.notna(first_comment_idx) and first_comment_idx in df.index:\n",
    "    print(f\"(Ticket index {first_comment_idx})\")\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    temp_idx = df[df[comments_col].fillna('').str.len() > 0].index.min()\n",
    "    if pd.notna(temp_idx):\n",
    "        print(f\"(Ticket index {temp_idx})\")\n",
    "        print(df.loc[temp_idx, 'cleaned_comments'])\n",
    "    else:\n",
    "        print(\"No comments found to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying final whitespace normalization pass...\n",
      "\n",
      "Text after FINAL whitespace normalization (first 2 rows):\n",
      "Cleaned Summary:\n",
      "0                                                                        emfdev add coupon benefit\n",
      "1    tata digital prod 1000006 tier downgrade job saves wrong note for tier renew downgrade reason\n",
      "\n",
      "Cleaned Description:\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "1    hi team we have received request from tata to explain on how to know if customer is getting renewed or downgraded for which reason. we know that for upgrades via system we get proper notes added as reason for upgrade. this has been verified and comments are accurate. while validating same for renew and downgrade cases we observed that notes are having different value but customer get renew or downgrade for different reason. impacted data sample - export job id - attached exported data - tierchangelogsampleupdated02.zip program titan upgrade downgrade - based on loyalty points earned in last 2 years. points tracker used sample records here customer upgrade downgrade renew happened via strategy based on points tracker. for upgrade cases comments are correct but for renew downgrade cases the reason comment is not correct. we need correct reason for actions renew downgrade based on which the action was taken. datedate timetime customerslabserialno customerslabslabname previouscustomerslabserialno previouscustomerslabslabname slabupgradeeventtypeupgradeeventname slabupgradeeventtypeupgradeeventcategory latestupdateddatedate latestupdatedtimetime usercustomerexternalid eventprogramprogramname eventprogramprogramid slabchangeactionslabchangeaction slabchangesourceslabchangesource notes user_id id 17032025 01 15 00 1 silver 2 gold customerregistration customerregistration 17032025 01 15 00 dd3ddb0bb5c22ba65ade5d3f2a55e576 titan 1000087 downgrade strategy pointsenginetier_downgrade visits 0 purchase 0.000 6535282 489106370 14032025 01 15 00 2 gold 2 gold customerregistration customerregistration 14032025 01 16 00 40e24b06415ace8e44b3f734171c67dc titan 1000087 renewal strategy pointsenginetier_downgrade visits 2 purchase 192138.62 33911032 489047601 17032025 01 15 00 1 silver 2 gold customerregistration customerregistration 17032025 01 15 00 5ba624e0df6032e23d6d4c6bc06f54ca titan 1000087 downgrade strategy pointsenginetier_downgrade visits 2 purchase 6085.000 37668590 489106462 18032025 14 46 00 2 gold 1 silver invalid invalid 18032025 14 46 00 6a74b7b6fd832e8c6a07fbed9fed7ec6 titan 1000087 upgrade strategy upgrading to slab slab2 name gold description slab 2 criteria primary tracker id 1326 tracker value 0 39747802 489118372 export job and file is attached. same reason also shows in membercare.\n",
      "\n",
      "Cleaned Comments (example from first ticket with comments):\n",
      "(Ticket index 0)\n",
      "\n",
      "\n",
      "Phase 1 and Phase 2 text cleaning complete for summary, description, and comments.\n"
     ]
    }
   ],
   "source": [
    "import re # Just in case it's a new cell and re wasn't imported recently\n",
    "\n",
    "# --- Final Whitespace Normalization Pass ---\n",
    "\n",
    "# Ensure the normalize_whitespace function is defined\n",
    "# (If you're running this in a new cell, you might need to redefine it or ensure the cell where it's defined has been run)\n",
    "# For completeness, here it is again:\n",
    "def normalize_whitespace(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace multiple spaces, tabs, and newlines (any whitespace sequence) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Trim leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "print(\"Applying final whitespace normalization pass...\")\n",
    "\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(normalize_whitespace)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(normalize_whitespace)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(normalize_whitespace)\n",
    "\n",
    "print(\"\\nText after FINAL whitespace normalization (first 2 rows):\")\n",
    "print(\"Cleaned Summary:\")\n",
    "print(df['cleaned_summary'].head(2).to_string())\n",
    "print(\"\\nCleaned Description:\")\n",
    "print(df['cleaned_description'].head(2).to_string())\n",
    "print(\"\\nCleaned Comments (example from first ticket with comments):\")\n",
    "\n",
    "# Assuming 'comments_col' and 'first_comment_idx' are defined from previous steps\n",
    "if 'first_comment_idx' in locals() and pd.notna(first_comment_idx) and first_comment_idx in df.index:\n",
    "    print(f\"(Ticket index {first_comment_idx})\")\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    # Fallback to find any comment if first_comment_idx is not set or invalid\n",
    "    temp_idx = df[df[comments_col].fillna('').str.len() > 0].index.min()\n",
    "    if pd.notna(temp_idx):\n",
    "        print(f\"(Ticket index {temp_idx})\")\n",
    "        print(df.loc[temp_idx, 'cleaned_comments'])\n",
    "    else:\n",
    "        print(\"No comments found to display.\")\n",
    "\n",
    "print(\"\\nPhase 1 and Phase 2 text cleaning complete for summary, description, and comments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Ensure pandas display options are set to show full column width\n",
    "# # This is good practice if you're also looking at DataFrames,\n",
    "# # but for printing a single string, it's not strictly necessary.\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# # Check if the DataFrame is not empty and the column exists\n",
    "# if not df.empty and 'cleaned_comments' in df.columns:\n",
    "#     # Access the first item in the 'cleaned_comments' Series\n",
    "#     first_cleaned_comment = df['cleaned_comments'].iloc[0]\n",
    "    \n",
    "#     print(\"Full content of the first 'cleaned_comments' entry:\")\n",
    "#     print(first_cleaned_comment)\n",
    "# else:\n",
    "#     if df.empty:\n",
    "#         print(\"The DataFrame 'df' is empty.\")\n",
    "#     else:\n",
    "#         print(\"Column 'cleaned_comments' not found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Step 9.6 (Corrected for Warnings & Logic): Remove Domain-Specific Data Patterns...\n",
      "\n",
      "Text after Step 9.6 (Corrected for Warnings & Logic - Remove Domain-Specific Data Patterns):\n",
      "\n",
      "Example of 'cleaned_comments' for ticket index 0:\n",
      "\n",
      "\n",
      "--- Processing Your Specific Example Text ---\n",
      "fyi verma reddy can you please share more details here like promotion name target event details and impacted users if it is aware. reddy yadav could you please help me baldi i m pulling the list of users promotion ids target ids. will add to ticket by afternoon baldi kailasanathan i updated the ticket with a sheet. of 3 cases only 1 is ready to be picked. the other 2 need config fixed first and i see that it is still not closed. i will check with implementation team and get back with an eta. reddy i don t have access to the sheet looking at the ticket description this looks to be at target loyalty. can you please share the access with everyone kailasanathan done kailasanathan config change is doen for below orgs and can be picked for cleanup.. once the cleanup script is ready and implemented for these 3 orgs we can do the cleanup for rest of them.ready for cleanup orglumen 9000209roche 9000284northside - 9000248cc reddy the case 4 in the sheet mentions update target achieved value for user-target .usertarget table s achieved value stores the sum of tracked targetvalue of usertargeteventlog entries. basically there is one to many relationship between usertarget and usertargeteventlog.please let me know which entries of usertargeteventlog should be excluded from corresponding usertarget. then only i can update usertarget s achievedvalue targetachievedemfmessagesent columns and negate the targetachievedeventlog and unifiedtargeteventlog. case 2 - it mentions that target is achieved but incentives are not earned.i started with org 9000209 and found data discrepancy. the sheet for case2 contains users who already have points from the target completion events. i have mentioned already allocated points for users of org 9000209 in same sheet.please let me know what to do for these users as these users fall in case1. cleanup for case 12 and 16 is done. tiwari - for case 2 most likely user must have been awarded for one cycle only. can you verify for other cycle too reddy can you connect with sourabh once on the cleanup hi yadav in the cleanup sheet for case 2 usertarget id of corresponding cycle is also mentioned. hence i tried only the usertarget ids which are mentioned. if there are other cycles of users which needs to be checked then they will have different usertarget ids. can you get the correct usertarget ids added in the sheet reddy - can you confirm if the document has the updated data with correct usertargetid in place yadav yes but i can re-run and validate if any new user targets were added since sunday. will connect with saurabh we are waiting for approval from optum to run the cleanup here. reddy tiwari is there an update on the approval from optum and cleanup being run a timeline would be great for this as it is impacting roughly 20-30 open incidents. cc .sharif morgan - tiwari won t be able to help here. reddy yadav can help. verma cc. morgan there are many tickets linked to the stride config related clean ups. could you please expedite this between nitish and prateek cc baldi peterson verma please reach out to nitish and align with him directly. kailey is on ops team and is getting stuck as an inbetween. updtae optum has approved the clean up on friday. shared the final list for cleanup to sourabh. we should be able to execute this on tuesday once tiwari is back.cc peterson morgan verma baldi cleanup for case 1.1 1.2 and 2.2 are done. case 2.1 is in progress currently. hi nitish cleanup for case 2.1 is also complete.please verify once from your end.cc baldi verma hi reddy after replaying stride target completed events again for benco org for list following are the results following users didn t get the points because they have targetcompleted before promotion was issued to them. hence validation failed warn promotion with id 2095 was earned by the customer but falls outside the valid promotion expiry period. customerid issualdate 3504875 20250115 21 07 57 4314072 20250212 15 42 46 3460262 20250220 10 26 23 4704136 20250114 17 27 25 4887108 20250224 09 16 16 three users got points in first run which were present in but they are also present in . overall summary users who got points from 1st and 2nd run including customerid 4251288 4813126 4495592 4441551 4384604 3504875 4368652 4400329 4314072 3812372 4142994 3460262 4421770 4704136 4366648 4103750 4384604 4887108 4005907 users who didn t get points from 1st and 2nd run including. this has some customers who should be excluded as manual allocation was done and 5 customer from point 1. - customerid 4251288 4813126 4495592 4441551 4384604 3504875 4368652 4400329 4314072 3812372 4142994 3460262 4421770 4704136 4366648 4103750 4384604 4887108 4005907 -cc baldi tiwari awesome saurabh. thank you for the update. for the 5 users whose target completed event falls prior to promotion enrollment can we unenroll re-enroll fire bes to give them their reward. we normally would not do this but they also have genuine events that were received post promotion issual that would have achieved their target properly post issual if not for our config issue in jan that counted pre issual events. hi reddy can i update promotion issual date to 1st jan for these users and replay targetcompleted events. it will be quicker and less error prone can we revert the issual date back again after the firing reporting may be impacted if we change the issual date tiwari yeah i have done the cleanup in mentioned way and reverted the issual date back to original. points are awarded to remaining 5 customers of benco.now lumen org is remaining where choice reward was not present in payload. hi nitish for lumen 9000209 points are awarded to eligible customers after updating choice reward and promotion issualdate.for global atlantic points are awarded to one customer after updating choice reward and promotion issualdate.other details are mentioned in the sheet where points are not awarded. most of them have hit capping limit.everything seems to be done on this ticket closing it now. please let us know if anything else remains.cc baldi verma\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Step 9.6 (New): Remove Domain-Specific Data Patterns (Corrected for Warnings & Logic) ---\n",
    "\n",
    "def remove_domain_specific_data(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Pattern for data tokens (re-used from remove_id_data_blobs)\n",
    "    # The (?i) for null is fine here as it's self-contained in this definition.\n",
    "    data_token = r'(?:\\b\\d+(?:\\.\\d+)?\\b|\\b[a-zA-Z0-9_.-]{8,}\\b|(?i)\\bnull\\b)' # Reduced ID length to 8 for more matches\n",
    "    \n",
    "    # A sequence of at least 3 such data tokens to identify data lines/blobs\n",
    "    # This will be part of a multi-line pattern check\n",
    "    data_tokens_on_line = r'(?:' + data_token + r'\\s*){3,}' # At least 3 data tokens on a line\n",
    "\n",
    "    # 1. Handle \"customerid issualdate\" and similar data headers followed by data lines\n",
    "    #    Also, handle lines that are just lists of data tokens.\n",
    "    #    This pattern looks for the header, then captures the following lines if they look like data.\n",
    "    header_pattern_str = r'(?:^customerid\\s*(?:issualdate\\s*)?.*?$)' # Header line, case-insensitive via flag\n",
    "    # A line that is predominantly data tokens\n",
    "    data_line_pattern_str = r'^\\s*' + data_tokens_on_line + r'\\s*$'\n",
    "\n",
    "    # Combine: Look for a header OR a data line, and if it's a data line,\n",
    "    # or if a header is followed by data lines, replace.\n",
    "    # This is tricky. Let's try to identify consecutive lines of data tokens first.\n",
    "    \n",
    "    # Iteratively find blocks of 2 or more consecutive lines that match data_line_pattern_str\n",
    "    # This is a placeholder for a more complex block-finding logic if needed.\n",
    "    # For now, let's simplify: if a line IS a data_line_pattern_str, replace it.\n",
    "    text = re.sub(data_line_pattern_str, ' <DATA_BLOB> ', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    # Then, if a header is followed by <DATA_BLOB>, consolidate or just ensure header is also gone if it was separate\n",
    "    text = re.sub(header_pattern_str + r'\\n(\\s*<DATA_BLOB>\\s*)', ' <DATA_BLOB> ', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    # If header didn't have data blob immediately after, but was on its own and we want to remove it too (less safe)\n",
    "    # text = re.sub(header_pattern_str, ' <DATA_HEADER_REMOVED> ', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "    # 2. Handle Rule Expressions / Configuration Details\n",
    "    rule_keywords = r'(?:currentcustomer\\.|currentevent\\.|rule\\d+|ruleset\\d+|actiontop\\d+)'\n",
    "    # A line that seems to be predominantly rule/log like\n",
    "    # If a line has at least two rule_keywords or one and looks like a typical assignment/check:\n",
    "    complex_rule_line_str = r'^\\s*' + rule_keywords + r'[^=\\n]+(?:=|isbefore|isafter|contains)[^\\n]+$'\n",
    "    text = re.sub(complex_rule_line_str, ' <RULE_CONFIGURATION> ', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    # Simpler: Replace lines that have \"request id\" followed by a long string\n",
    "    text = re.sub(r'^\\s*request id\\s+[a-f0-9]{20,}.*?$', ' <LOG_IDENTIFIERS> ', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "    # Consolidate multiple generated tokens and normalize whitespace\n",
    "    # Added re.IGNORECASE here just in case tokens somehow got cased, though they shouldn't.\n",
    "    text = re.sub(r'(\\s*<(?:DATA_BLOB|RULE_CONFIGURATION|LOG_IDENTIFIERS)>\\s*)+', r' \\1 ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- Apply this new step ---\n",
    "print(\"Applying Step 9.6 (Corrected for Warnings & Logic): Remove Domain-Specific Data Patterns...\")\n",
    "\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(remove_domain_specific_data)\n",
    "# df['cleaned_summary'] = df['cleaned_summary'].apply(remove_domain_specific_data)\n",
    "# df['cleaned_description'] = df['cleaned_description'].apply(remove_domain_specific_data)\n",
    "\n",
    "print(\"\\nText after Step 9.6 (Corrected for Warnings & Logic - Remove Domain-Specific Data Patterns):\")\n",
    "pd.set_option('display.max_colwidth', None) # Ensure full text is shown\n",
    "\n",
    "if 'first_comment_idx' in locals() and pd.notna(first_comment_idx) and first_comment_idx in df.index:\n",
    "    print(f\"\\nExample of 'cleaned_comments' for ticket index {first_comment_idx}:\")\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    temp_idx = df[df[comments_col].fillna('').str.len() > 0].index.min() # Ensure comments_col defined\n",
    "    if pd.notna(temp_idx):\n",
    "        print(f\"(Ticket index {temp_idx})\")\n",
    "        print(df.loc[temp_idx, 'cleaned_comments'])\n",
    "    else:\n",
    "        print(\"No comments found to display.\")\n",
    "\n",
    "# --- Test with your specific example text ---\n",
    "your_example_text = \"\"\"fyi verma reddy can you please share more details here like promotion name target event details and impacted users if it is aware. reddy yadav could you please help me baldi i m pulling the list of users promotion ids target ids. will add to ticket by afternoon baldi kailasanathan i updated the ticket with a sheet. of 3 cases only 1 is ready to be picked. the other 2 need config fixed first and i see that it is still not closed. i will check with implementation team and get back with an eta. reddy i don t have access to the sheet looking at the ticket description this looks to be at target loyalty. can you please share the access with everyone kailasanathan done kailasanathan config change is doen for below orgs and can be picked for cleanup.. once the cleanup script is ready and implemented for these 3 orgs we can do the cleanup for rest of them.ready for cleanup orglumen 9000209roche 9000284northside - 9000248cc reddy the case 4 in the sheet mentions update target achieved value for user-target .usertarget table s achieved value stores the sum of tracked targetvalue of usertargeteventlog entries. basically there is one to many relationship between usertarget and usertargeteventlog.please let me know which entries of usertargeteventlog should be excluded from corresponding usertarget. then only i can update usertarget s achievedvalue targetachievedemfmessagesent columns and negate the targetachievedeventlog and unifiedtargeteventlog. case 2 - it mentions that target is achieved but incentives are not earned.i started with org 9000209 and found data discrepancy. the sheet for case2 contains users who already have points from the target completion events. i have mentioned already allocated points for users of org 9000209 in same sheet.please let me know what to do for these users as these users fall in case1. cleanup for case 12 and 16 is done. tiwari - for case 2 most likely user must have been awarded for one cycle only. can you verify for other cycle too reddy can you connect with sourabh once on the cleanup hi yadav in the cleanup sheet for case 2 usertarget id of corresponding cycle is also mentioned. hence i tried only the usertarget ids which are mentioned. if there are other cycles of users which needs to be checked then they will have different usertarget ids. can you get the correct usertarget ids added in the sheet reddy - can you confirm if the document has the updated data with correct usertargetid in place yadav yes but i can re-run and validate if any new user targets were added since sunday. will connect with saurabh we are waiting for approval from optum to run the cleanup here. reddy tiwari is there an update on the approval from optum and cleanup being run a timeline would be great for this as it is impacting roughly 20-30 open incidents. cc .sharif morgan - tiwari won t be able to help here. reddy yadav can help. verma cc. morgan there are many tickets linked to the stride config related clean ups. could you please expedite this between nitish and prateek cc baldi peterson verma please reach out to nitish and align with him directly. kailey is on ops team and is getting stuck as an inbetween. updtae optum has approved the clean up on friday. shared the final list for cleanup to sourabh. we should be able to execute this on tuesday once tiwari is back.cc peterson morgan verma baldi cleanup for case 1.1 1.2 and 2.2 are done. case 2.1 is in progress currently. hi nitish cleanup for case 2.1 is also complete.please verify once from your end.cc baldi verma hi reddy after replaying stride target completed events again for benco org for list following are the results following users didn t get the points because they have targetcompleted before promotion was issued to them. hence validation failed warn promotion with id 2095 was earned by the customer but falls outside the valid promotion expiry period. customerid issualdate 3504875 20250115 21 07 57 4314072 20250212 15 42 46 3460262 20250220 10 26 23 4704136 20250114 17 27 25 4887108 20250224 09 16 16 three users got points in first run which were present in but they are also present in . overall summary users who got points from 1st and 2nd run including customerid 4251288 4813126 4495592 4441551 4384604 3504875 4368652 4400329 4314072 3812372 4142994 3460262 4421770 4704136 4366648 4103750 4384604 4887108 4005907 users who didn t get points from 1st and 2nd run including. this has some customers who should be excluded as manual allocation was done and 5 customer from point 1. - customerid 4251288 4813126 4495592 4441551 4384604 3504875 4368652 4400329 4314072 3812372 4142994 3460262 4421770 4704136 4366648 4103750 4384604 4887108 4005907 -cc baldi tiwari awesome saurabh. thank you for the update. for the 5 users whose target completed event falls prior to promotion enrollment can we unenroll re-enroll fire bes to give them their reward. we normally would not do this but they also have genuine events that were received post promotion issual that would have achieved their target properly post issual if not for our config issue in jan that counted pre issual events. hi reddy can i update promotion issual date to 1st jan for these users and replay targetcompleted events. it will be quicker and less error prone can we revert the issual date back again after the firing reporting may be impacted if we change the issual date tiwari yeah i have done the cleanup in mentioned way and reverted the issual date back to original. points are awarded to remaining 5 customers of benco.now lumen org is remaining where choice reward was not present in payload. hi nitish for lumen 9000209 points are awarded to eligible customers after updating choice reward and promotion issualdate.for global atlantic points are awarded to one customer after updating choice reward and promotion issualdate.other details are mentioned in the sheet where points are not awarded. most of them have hit capping limit.everything seems to be done on this ticket closing it now. please let us know if anything else remains.cc baldi verma\"\"\"\n",
    "print(\"\\n--- Processing Your Specific Example Text ---\")\n",
    "processed_example = remove_domain_specific_data(your_example_text)\n",
    "print(processed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Step 9.7: Remove/Replace Numbers...\n",
      "\n",
      "Text after Step 9.7 (Remove/Replace Numbers):\n",
      "\n",
      "Example of 'cleaned_comments' for ticket index 0:\n",
       "\n",
       "\n",
      "--- Processing Your Number-Heavy Example Text (After Number Removal) ---\n",
      "aying stride target completed events again for benco org for list following are the results following users didn t get the points because they have targetcompleted before promotion was issued to them. hence validation failed warn promotion with id was earned by the customer but falls outside the valid promotion expiry period. customerid issualdate three users got points in first run which were present in but they are also present in . overall summary users who got points from 1st and 2nd run including customerid users who didn t get points from 1st and 2nd run including. this has some customers who should be excluded as manual allocation was done and customer from point . - customerid -cc baldi tiwari awesome saurabh. thank you for the update. for the users whose target completed event falls prior to promotion enrollment can we unenroll re-enroll fire bes to give them their reward. we normally would not do this but they also have genuine events that were received post promotion issual that would have achieved their target properly post issual if not for our config issue in jan that counted pre issual events. hi reddy can i update promotion issual date to 1st jan for these users and replay targetcompleted events. it will be quicker and less error prone can we revert the issual date back again after the firing reporting may be impacted if we change the issual date tiwari yeah i have done the cleanup in mentioned way and reverted the issual date back to original. points are awarded to remaining customers of benco.now lumen org is remaining where choice reward was not present in payload. hi nitish for lumen points are awarded to eligible customers after updating choice reward and promotion issualdate.for global atlantic points are awarded to one customer after updating choice reward and promotion issualdate.other details are mentioned in the sheet where points are not awarded. most of them have hit capping limit.everything seems to be done on this ticket closing it now. please let us know if anything else remains.cc baldi verma\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Step 9.7: Remove/Replace Numbers (while trying to preserve version-like patterns) ---\n",
    "\n",
    "def remove_or_replace_numbers(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Protect common version-like patterns (e.g., v1.2.3, 1.2.3.4, project-1.2)\n",
    "    #    We'll replace them with a unique placeholder, then add them back later.\n",
    "    version_patterns = [\n",
    "        r'\\b\\d+\\.\\d+\\.\\d+(?:\\.\\d+)?\\b',  # e.g., 1.2.3 or 1.2.3.4\n",
    "        r'\\bv\\d+\\.\\d+(?:\\.\\d+)?\\b',      # e.g., v1.2 or v1.2.3\n",
    "        r'\\b[a-zA-Z_][a-zA-Z0-9_]*-\\d+\\.\\d+\\b' # e.g., project-1.2 (simple)\n",
    "    ]\n",
    "    \n",
    "    protected_versions = []\n",
    "    placeholder_base = \"||VERSION_PLACEHOLDER_{}||\"\n",
    "\n",
    "    for i, pattern in enumerate(version_patterns):\n",
    "        matches = re.finditer(pattern, text)\n",
    "        for match in reversed(list(matches)): # Iterate in reverse to handle indices correctly during replacement\n",
    "            placeholder = placeholder_base.format(len(protected_versions))\n",
    "            protected_versions.append(match.group(0))\n",
    "            start, end = match.span()\n",
    "            text = text[:start] + placeholder + text[end:]\n",
    "            \n",
    "    # 2. Remove standalone numbers (integers or floats not part of a larger word structure)\n",
    "    #    \\b ensures we match whole numbers, not digits within words.\n",
    "    #    This will remove numbers like \"2095\", \"3504875\", \"07\", \"5\" etc.\n",
    "    #    It will also remove numbers like \"1.1\", \"2.2\" if they weren't caught by version patterns.\n",
    "    text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\b', ' ', text) # Replace with space to separate words\n",
    "\n",
    "    # 3. Restore protected versions\n",
    "    for i, version_str in enumerate(reversed(protected_versions)): # Restore in reverse order of placeholder creation\n",
    "        placeholder = placeholder_base.format(len(protected_versions) - 1 - i)\n",
    "        text = text.replace(placeholder, version_str, 1) # Replace only the first occurrence\n",
    "\n",
    "    # Normalize whitespace again\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- Apply this new step ---\n",
    "print(\"Applying Step 9.7: Remove/Replace Numbers...\")\n",
    "\n",
    "# Apply to the cleaned_comments (and other fields if desired)\n",
    "df['cleaned_comments'] = df['cleaned_comments'].apply(remove_or_replace_numbers)\n",
    "# df['cleaned_summary'] = df['cleaned_summary'].apply(remove_or_replace_numbers)\n",
    "# df['cleaned_description'] = df['cleaned_description'].apply(remove_or_replace_numbers)\n",
       "\n",
       "\n",
    "print(\"\\nText after Step 9.7 (Remove/Replace Numbers):\")\n",
    "pd.set_option('display.max_colwidth', None) # Ensure full text view\n",
       "\n",
    "if 'first_comment_idx' in locals() and pd.notna(first_comment_idx) and first_comment_idx in df.index:\n",
    "    print(f\"\\nExample of 'cleaned_comments' for ticket index {first_comment_idx}:\")\n",
    "    print(df.loc[first_comment_idx, 'cleaned_comments'])\n",
    "else:\n",
    "    temp_idx = df[df[comments_col].fillna('').str.len() > 0].index.min() # Ensure comments_col defined\n",
    "    if pd.notna(temp_idx):\n",
    "        print(f\"(Ticket index {temp_idx})\")\n",
    "        print(df.loc[temp_idx, 'cleaned_comments'])\n",
    "    else:\n",
    "        print(\"No comments found to display.\")\n",
       "\n",
    "# --- Test with your specific example text that contained many numbers ---\n",
    "your_number_heavy_text = \"\"\"aying stride target completed events again for benco org for list following are the results following users didn t get the points because they have targetcompleted before promotion was issued to them. hence validation failed warn promotion with id 2095 was earned by the customer but falls outside the valid promotion expiry period. customerid issualdate 3504875 20250115 21 07 57 4314072 20250212 15 42 46 3460262 20250220 10 26 23 4704136 20250114 17 27 25 4887108 20250224 09 16 16 three users got points in first run which were present in but they are also present in . overall summary users who got points from 1st and 2nd run including customerid 4251288 4813126 4495592 4441551 4384604 3504875 4368652 4400329 4314072 3812372 4142994 3460262 4421770 4704136 4366648 4103750 4384604 4887108 4005907 users who didn t get points from 1st and 2nd run including. this has some customers who should be excluded as manual allocation was done and 5 customer from point 1. - customerid 4251288 4813126 4495592 4441551 4384604 3504875 4368652 4400329 4314072 3812372 4142994 3460262 4421770 4704136 4366648 4103750 4384604 4887108 4005907 -cc baldi tiwari awesome saurabh. thank you for the update. for the 5 users whose target completed event falls prior to promotion enrollment can we unenroll re-enroll fire bes to give them their reward. we normally would not do this but they also have genuine events that were received post promotion issual that would have achieved their target properly post issual if not for our config issue in jan that counted pre issual events. hi reddy can i update promotion issual date to 1st jan for these users and replay targetcompleted events. it will be quicker and less error prone can we revert the issual date back again after the firing reporting may be impacted if we change the issual date tiwari yeah i have done the cleanup in mentioned way and reverted the issual date back to original. points are awarded to remaining 5 customers of benco.now lumen org is remaining where choice reward was not present in payload. hi nitish for lumen 9000209 points are awarded to eligible customers after updating choice reward and promotion issualdate.for global atlantic points are awarded to one customer after updating choice reward and promotion issualdate.other details are mentioned in the sheet where points are not awarded. most of them have hit capping limit.everything seems to be done on this ticket closing it now. please let us know if anything else remains.cc baldi verma\"\"\"\n",
    "print(\"\\n--- Processing Your Number-Heavy Example Text (After Number Removal) ---\")\n",
    "processed_number_example = remove_or_replace_numbers(your_number_heavy_text)\n",
    "# To see the effect more clearly, you might want to apply ALL previous cleaning steps to your_number_heavy_text first,\n",
    "# then apply remove_or_replace_numbers.\n",
    "# For a quick test, just applying this function will show if it removes the numbers.\n",
    "print(processed_number_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in your DataFrame:\n",
      "Index(['ticket_id', 'summary', 'description', 'status', 'priority', 'reporter',\n",
      "       'assignee', 'created_at', 'updated_at', 'labels', 'components',\n",
      "       'owned_by_team', 'brand', 'product', 'geo_region', 'environment',\n",
      "       'root_cause', 'sprint', 'comments', 'url', 'issue_type',\n",
      "       'cleaned_summary', 'cleaned_description', 'cleaned_comments'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Column names in your DataFrame:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
       "\n",
      "Current shape of the DataFrame (rows, columns): (10, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCurrent shape of the DataFrame (rows, columns): {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
     "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments with more than 200 words: 1\n",
      "\n",
      "Word count statistics:\n",
      "Mean word count: 116.4\n",
      "Median word count: 23.5\n",
      "Maximum word count: 442\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'cleaned_comments' is the column with the cleaned text.\n",
    "# If 'cleaned_comments' might have NaN values (though our pipeline tries to make them empty strings),\n",
    "# fillna('') ensures the .split() method doesn't error out.\n",
    "# .str.split() splits the string in each cell by whitespace by default, resulting in a list of words.\n",
    "# .str.len() then gets the length of each of these lists (i.e., the word count for that cell).\n",
    "# .max() finds the maximum value in the resulting Series of word counts.\n",
    "\n",
    "if 'cleaned_comments' in df.columns:\n",
    "    # Calculate word counts for each comment\n",
    "    word_counts = df['cleaned_comments'].fillna('').str.split().str.len()\n",
    "    \n",
    "    # Count comments with more than 200 words\n",
    "    long_comments_count = len(word_counts[word_counts > 400])\n",
    "    print(f\"Number of comments with more than 200 words: {long_comments_count}\")\n",
    "    \n",
    "    # Optional: Display some statistics\n",
    "    print(\"\\nWord count statistics:\")\n",
    "    print(f\"Mean word count: {word_counts.mean():.1f}\")\n",
    "    print(f\"Median word count: {word_counts.median():.1f}\")\n",
    "    print(f\"Maximum word count: {word_counts.max()}\")\n",
    "else:\n",
    "    print(\"Column 'cleaned_comments' not found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
